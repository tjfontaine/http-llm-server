# MIT License
#
# Copyright (c) 2025 Timothy J Fontaine
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in all
# copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

import http.cookies
import http.server
import json
import logging
import time
from email.utils import formatdate

import jinja2
from agents import (
    Runner,
)
from agents.items import MessageOutputItem, ToolCallItem
from agents.stream_events import RawResponsesStreamEvent, RunItemStreamEvent
from aiohttp import web
from openai.types.responses import ResponseFunctionToolCall

from .local_tools import create_local_tools_stdio_server
from src.config import Config
from .logging_config import configure_logging, get_loggers
from .server.session import AbstractSessionStore, InMemorySessionStore
from .server.parsing import get_raw_request_aiohttp
from .server.agent_setup import initialize_mcp_servers_and_agent


# Default web app file path
DEFAULT_WEB_APP_FILE = "examples/default_info_site/prompt.md"


# --- Main Application ---
# --- Static Prompts and Global State ---
LLM_HTTP_SERVER_PROMPT_BASE = ""  # This will be loaded from a file


# This is loaded by main.py and passed in the config
# ERROR_LLM_SYSTEM_PROMPT_TEMPLATE = ""


# --- Logging Configuration (Global for simplicity, initialized early) ---
# Initialize with default logging - will be reconfigured when config is available
app_logger, access_logger, conversation_logger = get_loggers()


async def _send_llm_error_response_aiohttp(
    request: web.Request, status_code: int, message: str, error_details: str = ""
) -> web.Response:
    """
    Sends an error response generated by an LLM.

    If the LLM fails, it sends a minimal plain text response as a fallback.
    """
    ERROR_LLM_SYSTEM_PROMPT_TEMPLATE = request.app["error_llm_system_prompt_template"]

    async def _minimal_fallback_response():
        # This is the last resort, plain text response
        fallback_body = (
            f"HTTP {status_code} - {message}\n\nError Details: {error_details}"
        )
        return web.Response(
            text=fallback_body,
            status=status_code,
            content_type="text/plain",
            charset="utf-8",
            headers={"Connection": "close"},
        )

    agent = request.app.get("agent")
    web_app_rules = request.app.get("web_app_rules", "A generic web application.")

    if not agent:
        app_logger.warning(
            "Agent not available for LLM-generated error page. Falling back to minimal plain text."
        )
        return await _minimal_fallback_response()

    try:
        app_logger.info(
            f"Attempting to generate a styled error page with LLM for status {status_code}..."
        )

        template = jinja2.Template(ERROR_LLM_SYSTEM_PROMPT_TEMPLATE)
        error_system_prompt = template.render(
            status_code=status_code,
            message=message,
            error_details=error_details,
            web_app_rules=web_app_rules,
        )

        messages = [
            {"role": "system", "content": error_system_prompt},
            {
                "role": "user",
                "content": f"Please generate the HTTP response for the {status_code} error page now.",
            },
        ]

        output_item = await Runner.run(agent, messages)

        if not isinstance(output_item, MessageOutputItem) or not output_item.content:
            app_logger.error(
                "LLM did not return a valid MessageOutputItem for the error page. Falling back to minimal plain text."
            )
            return await _minimal_fallback_response()

        llm_response_text = output_item.content
        app_logger.info("Successfully received styled error page from LLM.")

        separator = None
        if "\r\n\r\n" in llm_response_text:
            separator = "\r\n\r\n"
        elif "\n\n" in llm_response_text:
            separator = "\n\n"

        if not separator:
            app_logger.error(
                "LLM-generated error page response is missing header-body separator. Falling back to minimal plain text."
            )
            return await _minimal_fallback_response()

        header_section, body = llm_response_text.split(separator, 1)

        lines = header_section.split("\n")
        llm_headers = {}
        if lines:
            for line in lines[1:]:
                if ":" in line:
                    key, value = line.split(":", 1)
                    llm_headers[key.strip()] = value.strip()

        return web.Response(
            text=body,
            status=status_code,
            content_type=llm_headers.get("Content-Type", "text/html; charset=utf-8"),
            headers=llm_headers,
        )

    except Exception as e:
        app_logger.exception(
            f"Failed to generate LLM error page, falling back to minimal plain text template: {e}"
        )
        return await _minimal_fallback_response()


async def handle_http_request(request: web.Request) -> web.StreamResponse:
    start_time = time.perf_counter()
    client_address_tuple = request.transport.get_extra_info("peername")
    client_address_str = (
        f"{client_address_tuple[0]}:{client_address_tuple[1]}"
        if client_address_tuple
        else "Unknown Client"
    )
    access_logger.info(
        f"[{client_address_str}] Incoming request: {request.method} {request.path_qs}"
    )

    current_session_store: AbstractSessionStore = request.app["session_store"]
    raw_request_text = await get_raw_request_aiohttp(request)

    session_id_from_cookie = None
    cookie_header = request.headers.get("Cookie")
    if cookie_header:
        try:
            cookies = http.cookies.SimpleCookie()
            cookies.load(cookie_header)
            if "X-Chat-Session-ID" in cookies:
                session_id_from_cookie = cookies["X-Chat-Session-ID"].value
                if session_id_from_cookie:
                    app_logger.info(
                        f"[{client_address_str}] Existing session ID found in cookie: {session_id_from_cookie}"
                    )
        except Exception:
            app_logger.exception(
                f"[{client_address_str}] Error parsing 'Cookie' header: '{cookie_header}'. Treating as no session."
            )

    # Load typed config
    config = request.app["config"]
    system_prompt_template = config.system_prompt_template
    agent = request.app["agent"]
    global_state = request.app["global_state"]
    max_turns = config.max_turns
    context_window_max = config.context_window_max

    history = []
    current_token_count = 0
    if session_id_from_cookie:
        full_history = await current_session_store.get_history(session_id_from_cookie)
        # Strip extra keys from history that the LLM API might reject
        history = [
            {"role": turn["role"], "content": turn["content"]}
            for turn in full_history.messages
        ]
        current_token_count = await current_session_store.get_token_count(
            session_id_from_cookie
        )

    jinja_context = {
        "session_id": session_id_from_cookie or "",
        "global_state": json.dumps(global_state),
        "current_token_count": str(current_token_count),
        "context_window_max": str(context_window_max),
        "dynamic_date_example": formatdate(timeval=None, localtime=False, usegmt=True),
        "dynamic_server_name_example": "LLMWebServer/0.1",
    }

    try:
        template = jinja2.Template(system_prompt_template)
        dynamic_system_prompt = template.render(jinja_context)
    except jinja2.exceptions.TemplateSyntaxError as e:
        app_logger.exception(f"Jinja2 template syntax error in the system prompt: {e}")
        return await _send_llm_error_response_aiohttp(
            request,
            500,
            "Server Configuration Error",
            "Invalid system prompt template.",
        )

    messages = [{"role": "system", "content": dynamic_system_prompt}]
    messages.extend(history)
    messages.append({"role": "user", "content": raw_request_text})

    app_logger.info(
        f"[{client_address_str}] Handing request to LLM with session context: "
        f"ID='{session_id_from_cookie or 'None'}', "
        f"HistoryTurns={len(history)}, TokenCount={current_token_count}"
    )

    agent.instructions = None

    llm_call_start_time = None
    llm_first_token_time = None
    llm_stream_end_time = None
    llm_response_fully_collected_text_for_log = ""
    model_error_indicator_for_recording = None
    _last_chunk_finish_reason = None
    prompt_tokens_from_usage = 0
    completion_tokens_from_usage = 0

    response = None
    # Use a local variable for session state to handle cases where the session ID is assigned mid-stream.
    final_session_id_for_turn = session_id_from_cookie

    try:
        llm_call_start_time = time.perf_counter()
        app_logger.debug(
            f"[{client_address_str}] Starting LLM request processing",
            extra={
                "session_id": session_id_from_cookie or "new",
                "history_turns": len(history),
                "token_count": current_token_count,
                "max_turns": max_turns,
            },
        )
        app_logger.info(f"[{client_address_str}] Processing LLM request...")

        agent_stream = Runner.run_streamed(
            agent,
            messages,
            max_turns=max_turns,
        )

        llm_first_token_time = time.perf_counter()

        response = web.StreamResponse()
        response.enable_chunked_encoding(chunk_size=None)

        headers_and_status_parsed = False
        body_buffer = ""

        # The core of this server: stream the agent's response.
        # This loop processes events from the agent, including tool calls and content chunks.
        # A key design element is that the LLM is expected to generate a raw HTTP response,
        # which this server parses on-the-fly to construct a valid `aiohttp.web.Response`.
        async for event in agent_stream.stream_events():
            if isinstance(event, RawResponsesStreamEvent):
                raw_chunk = event.data
                if (
                    hasattr(raw_chunk, "response")
                    and hasattr(raw_chunk.response, "usage")
                    and raw_chunk.response.usage
                ):
                    usage = raw_chunk.response.usage
                    app_logger.info(
                        f"[{client_address_str}] Usage found in stream chunk: {usage}"
                    )
                    prompt_tokens_from_usage += usage.input_tokens
                    completion_tokens_from_usage += usage.output_tokens

            if isinstance(event, RunItemStreamEvent):
                # Only log meaningful stream events at DEBUG level
                if event.name in [
                    "tool_called",
                    "tool_output",
                    "message_output_created",
                ]:
                    app_logger.debug(
                        f"[{client_address_str}] Stream event: {event.name}",
                        extra={"event_type": type(event.item).__name__},
                    )
                elif event.name in [
                    "message_chunk_created"
                ] and app_logger.isEnabledFor(logging.DEBUG):
                    # For message chunks, show content preview at DEBUG level
                    chunk_preview = ""
                    if hasattr(event.item, "chunk") and hasattr(
                        event.item.chunk, "text"
                    ):
                        chunk_preview = event.item.chunk.text[:50] + (
                            "..." if len(event.item.chunk.text) > 50 else ""
                        )
                    app_logger.debug(
                        f"[{client_address_str}] Received text chunk",
                        extra={
                            "content_preview": chunk_preview,
                            "chunk_length": len(event.item.chunk.text)
                            if hasattr(event.item, "chunk")
                            and hasattr(event.item.chunk, "text")
                            else 0,
                        },
                    )

                if event.name == "tool_called":
                    # Type-safe check for tool calls
                    if isinstance(event.item, ToolCallItem) and isinstance(
                        event.item.raw_item, ResponseFunctionToolCall
                    ):
                        tool_call = event.item.raw_item
                        tool_name = tool_call.name
                        tool_args = {}
                        if tool_call.arguments:
                            try:
                                tool_args = json.loads(tool_call.arguments)
                            except json.JSONDecodeError:
                                app_logger.warning(
                                    f"Could not decode tool arguments: {tool_call.arguments}",
                                    extra={
                                        "tool_name": tool_name,
                                        "error": "Skipping tool call with no valid arguments",
                                    },
                                )
                                continue

                        # Log tool call with structured information
                        app_logger.debug(
                            f"[{client_address_str}] LLM calling tool: {tool_name}",
                            extra={
                                "tool_name": tool_name,
                                "args_count": len(tool_args),
                                "key_args": str(
                                    {
                                        k: str(v)[:50]
                                        + ("..." if len(str(v)) > 50 else "")
                                        for k, v in list(tool_args.items())[:3]
                                    }
                                )
                                if tool_args
                                else "none",
                            },
                        )

                        if tool_name == "assign_session_id":
                            new_id = tool_args.get("session_id")
                            if new_id:
                                app_logger.info(
                                    f"Session ID '{new_id}' assigned via tool call. Adopting for logging."
                                )
                                final_session_id_for_turn = new_id

                if event.name in [
                    "message_chunk_created",
                    "message_output_created",
                ]:
                    chunk = ""
                    item = event.item
                    if hasattr(item, "chunk"):
                        if hasattr(item.chunk, "text"):
                            chunk = item.chunk.text
                    elif hasattr(item, "raw_item"):
                        if isinstance(item.raw_item.content, list):
                            for part in item.raw_item.content:
                                if hasattr(part, "text"):
                                    chunk = part.text
                                    break
                        elif hasattr(item.raw_item, "content"):
                            chunk = str(item.raw_item.content)

                    if not chunk:
                        continue

                    llm_response_fully_collected_text_for_log += chunk

                    if headers_and_status_parsed:
                        await response.write(chunk.encode("utf-8"))
                        continue

                    body_buffer += chunk

                    # The LLM is expected to stream a full HTTP response, headers and body.
                    # We buffer the initial part of the stream until we find the double newline
                    # that separates headers from the body.
                    separator = None
                    if "\r\n\r\n" in body_buffer:
                        separator = "\r\n\r\n"
                    elif "\n\n" in body_buffer:
                        separator = "\n\n"

                    if separator:
                        header_section, body_part = body_buffer.split(separator, 1)
                        headers_and_status_parsed = True

                        lines = header_section.split("\n")
                        llm_status_code = 200
                        llm_headers = {}
                        if lines:
                            status_line = lines[0].strip()
                            if status_line.startswith("HTTP/"):
                                try:
                                    parts = status_line.split(" ", 2)
                                    if len(parts) >= 2:
                                        llm_status_code = int(parts[1])
                                except (ValueError, IndexError):
                                    app_logger.warning(
                                        f"Invalid status line: {status_line}"
                                    )
                            for line in lines[1:]:
                                if ":" in line:
                                    key, value = line.split(":", 1)
                                    llm_headers[key.strip()] = value.strip()

                        response.set_status(llm_status_code)
                        for k, v in llm_headers.items():
                            response.headers[k] = v
                        await response.prepare(request)

                        app_logger.debug(
                            f"[{client_address_str}] Parsed HTTP response headers from LLM",
                            extra={
                                "status_code": llm_status_code,
                                "headers_count": len(llm_headers),
                                "content_type": llm_headers.get(
                                    "Content-Type", "unknown"
                                ),
                                "body_preview": body_part[:100]
                                + ("..." if len(body_part) > 100 else "")
                                if body_part
                                else "none",
                            },
                        )
                        app_logger.info(
                            f"[{client_address_str}] Parsed HTTP headers from LLM, streaming response."
                        )

                        if body_part:
                            await response.write(body_part.encode("utf-8"))
            else:
                # For RawResponsesStreamEvent and other events, only log if they contain useful info
                if isinstance(
                    event, RawResponsesStreamEvent
                ) and app_logger.isEnabledFor(logging.DEBUG):
                    # Extract useful information from raw response events
                    if hasattr(event, "item") and hasattr(event.item, "raw_item"):
                        # Look for finish reason, usage info, etc.
                        raw_item = event.item.raw_item
                        debug_info = {}
                        if (
                            hasattr(raw_item, "finish_reason")
                            and raw_item.finish_reason
                        ):
                            debug_info["finish_reason"] = raw_item.finish_reason
                        if hasattr(raw_item, "usage") and raw_item.usage:
                            debug_info["tokens"] = (
                                f"input:{raw_item.usage.input_tokens},output:{raw_item.usage.output_tokens}"
                            )

                        if debug_info:  # Only log if we have useful info
                            app_logger.debug(
                                f"[{client_address_str}] Raw response event with metadata",
                                extra=debug_info,
                            )
                elif not isinstance(event, RawResponsesStreamEvent):
                    # Log other event types that might be interesting
                    app_logger.debug(
                        f"[{client_address_str}] Stream event",
                        extra={"event_type": type(event).__name__},
                    )

        llm_stream_end_time = time.perf_counter()

        if not response.prepared:
            app_logger.warning(
                f"[{client_address_str}] LLM stream finished without a valid HTTP response header."
            )
            return await _send_llm_error_response_aiohttp(
                request,
                500,
                "Internal Server Error",
                "LLM did not produce a valid HTTP response.",
            )
        else:
            await response.write_eof()
            app_logger.info(
                f"[{client_address_str}] Successfully streamed full LLM response."
            )
        return response

    except Exception:
        app_logger.exception(
            f"[{client_address_str}] Unexpected error processing LLM stream:"
        )
        model_error_indicator_for_recording = "UNEXPECTED_STREAM_PROCESSING_ERROR"
        llm_response_fully_collected_text_for_log = "ERROR_UNEXPECTED_STREAM_PROCESSING"

        if response and not response.prepared:
            return await _send_llm_error_response_aiohttp(
                request,
                500,
                "Internal Server Error",
                "Unexpected error during stream processing.",
            )

        return response
    finally:
        # This block ensures that critical post-request actions are always performed,
        # such as recording the conversation turn and logging detailed performance metrics.
        # This is vital for debugging, monitoring, and maintaining session state.
        if final_session_id_for_turn:
            await current_session_store.record_turn(
                final_session_id_for_turn, "user", raw_request_text
            )

            assistant_content_for_history = llm_response_fully_collected_text_for_log
            if model_error_indicator_for_recording:
                assistant_content_for_history = f"[LLM_RESPONSE_STREAM_INTERRUPTED_OR_ERROR: {model_error_indicator_for_recording}]\n\n{llm_response_fully_collected_text_for_log}"
            elif not llm_response_fully_collected_text_for_log.strip():
                assistant_content_for_history = "[LLM_EMPTY_RESPONSE_STREAMED]"

            await current_session_store.record_turn(
                final_session_id_for_turn,
                "assistant",
                assistant_content_for_history,
            )
            if prompt_tokens_from_usage > 0:
                await current_session_store.update_token_count(
                    final_session_id_for_turn, prompt_tokens_from_usage
                )
        else:
            app_logger.error(
                f"[{client_address_str}] Could not determine session ID for saving conversation turn. "
                "LLM may have failed to create a session or set a cookie."
            )

        end_time = time.perf_counter()
        duration = end_time - start_time
        ttft_str = "N/A"
        duration_llm_stream_str = "N/A"

        llm_ttft_seconds_val = None
        if llm_call_start_time and llm_first_token_time:
            ttft_calc = llm_first_token_time - llm_call_start_time
            if ttft_calc >= 0:
                llm_ttft_seconds_val = ttft_calc
                ttft_str = f"{llm_ttft_seconds_val:.3f}s"

        llm_stream_duration_seconds_val = None
        if llm_call_start_time and llm_stream_end_time:
            duration_llm_calc = llm_stream_end_time - llm_call_start_time
            if duration_llm_calc >= 0:
                llm_stream_duration_seconds_val = duration_llm_calc
                duration_llm_stream_str = f"{llm_stream_duration_seconds_val:.3f}s"

        compl_tokens_per_sec_str = "N/A"
        compl_tokens_per_sec_val = None
        if llm_stream_duration_seconds_val is not None:
            if llm_stream_duration_seconds_val > 0:
                if completion_tokens_from_usage > 0:
                    tokens_per_sec = (
                        completion_tokens_from_usage / llm_stream_duration_seconds_val
                    )
                    compl_tokens_per_sec_str = f"{tokens_per_sec:.2f}"
                    compl_tokens_per_sec_val = tokens_per_sec
                else:
                    compl_tokens_per_sec_str = "0.00 (no tokens)"
                    compl_tokens_per_sec_val = 0.0
            elif llm_stream_duration_seconds_val == 0:
                if completion_tokens_from_usage > 0:
                    compl_tokens_per_sec_str = "Infinity"
                    compl_tokens_per_sec_val = float("inf")
                else:
                    compl_tokens_per_sec_str = "0.00 (no tokens, instantaneous)"
                    compl_tokens_per_sec_val = 0.0

        # Debug timing and response characteristics
        app_logger.debug(
            f"[{client_address_str}] Request processing complete",
            extra={
                "total_duration": f"{duration:.3f}s",
                "llm_ttft": ttft_str,
                "llm_stream_duration": duration_llm_stream_str,
                "tokens_per_second": compl_tokens_per_sec_str,
                "response_size_chars": len(llm_response_fully_collected_text_for_log),
                "session_id": final_session_id_for_turn or "none",
                "had_errors": bool(model_error_indicator_for_recording),
            },
        )

        access_log_extra = {
            "remote_address": client_address_str,
            "total_duration_seconds": round(duration, 3),
            "llm_ttft_seconds": round(llm_ttft_seconds_val, 3)
            if llm_ttft_seconds_val is not None
            else None,
            "llm_stream_duration_seconds": round(llm_stream_duration_seconds_val, 3)
            if llm_stream_duration_seconds_val is not None
            else None,
            "prompt_tokens": prompt_tokens_from_usage,
            "completion_tokens": completion_tokens_from_usage,
            "completion_tokens_per_second": round(compl_tokens_per_sec_val, 2)
            if compl_tokens_per_sec_val is not None
            and compl_tokens_per_sec_val != float("inf")
            else compl_tokens_per_sec_val,
            "session_hkey": final_session_id_for_turn,
            "session_log_id": final_session_id_for_turn,
            "new_session_by_server": final_session_id_for_turn
            != session_id_from_cookie,
            "http_method": request.method,
            "http_path_qs": request.path_qs,
            "llm_finish_reason": _last_chunk_finish_reason,
        }

        log_msg_final = f"[{client_address_str}] Request handled. "
        log_msg_final += f"TotalDur: {duration:.3f}s, LLM_TTFT: {ttft_str}, LLM_StreamDur: {duration_llm_stream_str}, "
        log_msg_final += f"PToken: {prompt_tokens_from_usage}, CToken: {completion_tokens_from_usage}, CTPS: {compl_tokens_per_sec_str}, "
        log_msg_final += f"Sess: {final_session_id_for_turn}, "
        log_msg_final += f"FinishReason: {_last_chunk_finish_reason if _last_chunk_finish_reason else 'N/A'}."

        if model_error_indicator_for_recording:
            access_log_extra["error_indicator"] = model_error_indicator_for_recording
            access_log_extra["llm_raw_response_on_error"] = (
                llm_response_fully_collected_text_for_log
            )
            log_msg_final += f" Error: {model_error_indicator_for_recording}."

        access_logger.info(log_msg_final, extra=access_log_extra)


async def on_startup(app: web.Application):
    """Initialize application state and connections."""
    config: Config = app["config"]
    app_logger.info("Server is starting up...")
    app["start_time"] = time.time()
    await initialize_mcp_servers_and_agent(config, app)
    app_logger.info("Server startup complete.")


async def on_shutdown(app: web.Application):
    """Async operations to perform on server shutdown."""
    app_logger.info("\nServer shutting down (async)...")

    # Disconnect from MCP servers
    if app.get("mcp_server_lifecycles"):
        app_logger.info(
            f"Closing {len(app['mcp_server_lifecycles'])} MCP server connections..."
        )
        for mcp_server in app["mcp_server_lifecycles"]:
            try:
                await mcp_server.close()
                app_logger.info(f"Closed MCP server: {mcp_server.params}")
            except Exception as e:
                app_logger.error(
                    f"Error closing MCP server {mcp_server.params}: {e}",
                    exc_info=True,
                )

    log_directory = "conversation_logs"
    current_session_store: AbstractSessionStore = app["session_store"]
    await current_session_store.save_all_sessions_on_shutdown(log_directory)

    app_logger.info("Server shutdown actions completed.")


def create_app(config: Config) -> web.Application:
    """
    Create and configure the main aiohttp application.
    """
    # Reconfigure logging with the specified level
    configure_logging(config.log_level)

    # Typed config from Pydantic
    session_store = InMemorySessionStore(save_to_disk=config.save_conversations)

    app = web.Application()
    app["global_state"] = {}
    app["config"] = config
    app["session_store"] = session_store
    app["error_llm_system_prompt_template"] = config.error_llm_system_prompt_template

    app.router.add_route("*", "/{path:.*}", handle_http_request)

    app.on_startup.append(on_startup)
    app.on_shutdown.append(on_shutdown)

    return app


def run_local_tools_stdio_server():
    """Entry point for running the local tools server as a stdio MCP server."""
    # This server runs in its own process and has its own independent state.
    app_logger.info("Starting local tools stdio server...")
    # State is not saved to disk for the subprocess.
    session_store = InMemorySessionStore(save_to_disk=False)
    global_state = {}
    tools_app = create_local_tools_stdio_server(global_state, session_store)

    # The StdioServer's run() method is async and will run until the process is terminated.
    try:
        tools_app.run(transport="stdio")
    except KeyboardInterrupt:
        app_logger.info("Local tools stdio server shut down by user.")
    finally:
        app_logger.info("Local tools stdio server has exited.")
