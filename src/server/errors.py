# MIT License
#
# Copyright (c) 2025 Timothy J Fontaine
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in all
# copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

"""
Error handling module for LLM-generated error responses.
"""

import jinja2
from agents import Runner
from agents.items import MessageOutputItem
from aiohttp import web

from ..logging_config import get_loggers

# Get logger for error handling
app_logger, _, _ = get_loggers()


async def send_llm_error_response_aiohttp(
    request: web.Request, status_code: int, message: str, error_details: str = ""
) -> web.Response:
    """
    Sends an error response generated by an LLM.

    If the LLM fails, it sends a minimal plain text response as a fallback.

    Args:
        request: The aiohttp request object
        status_code: HTTP status code for the error
        message: Error message to display
        error_details: Additional error details (optional)

    Returns:
        An aiohttp Response object with the error page
    """
    ERROR_LLM_SYSTEM_PROMPT_TEMPLATE = request.app["error_llm_system_prompt_template"]

    async def _minimal_fallback_response():
        # This is the last resort, plain text response
        fallback_body = (
            f"HTTP {status_code} - {message}\n\nError Details: {error_details}"
        )
        return web.Response(
            text=fallback_body,
            status=status_code,
            content_type="text/plain",
            charset="utf-8",
            headers={"Connection": "close"},
        )

    agent = request.app.get("agent")
    web_app_rules = request.app.get("web_app_rules", "A generic web application.")

    if not agent:
        app_logger.warning(
            "Agent not available for LLM-generated error page. Falling back to minimal plain text."
        )
        return await _minimal_fallback_response()

    try:
        app_logger.info(
            f"Attempting to generate a styled error page with LLM for status {status_code}..."
        )

        template = jinja2.Template(ERROR_LLM_SYSTEM_PROMPT_TEMPLATE)
        error_system_prompt = template.render(
            status_code=status_code,
            message=message,
            error_details=error_details,
            web_app_rules=web_app_rules,
        )

        messages = [
            {"role": "system", "content": error_system_prompt},
            {
                "role": "user",
                "content": f"Please generate the HTTP response for the {status_code} error page now.",
            },
        ]

        output_item = await Runner.run(agent, messages)

        if not isinstance(output_item, MessageOutputItem) or not output_item.content:
            app_logger.error(
                "LLM did not return a valid MessageOutputItem for the error page. Falling back to minimal plain text."
            )
            return await _minimal_fallback_response()

        llm_response_text = output_item.content
        app_logger.info("Successfully received styled error page from LLM.")

        separator = None
        if "\r\n\r\n" in llm_response_text:
            separator = "\r\n\r\n"
        elif "\n\n" in llm_response_text:
            separator = "\n\n"

        if not separator:
            app_logger.error(
                "LLM-generated error page response is missing header-body separator. Falling back to minimal plain text."
            )
            return await _minimal_fallback_response()

        header_section, body = llm_response_text.split(separator, 1)

        lines = header_section.split("\n")
        llm_headers = {}
        if lines:
            for line in lines[1:]:
                if ":" in line:
                    key, value = line.split(":", 1)
                    llm_headers[key.strip()] = value.strip()

        return web.Response(
            text=body,
            status=status_code,
            content_type=llm_headers.get("Content-Type", "text/html; charset=utf-8"),
            headers=llm_headers,
        )

    except Exception as e:
        app_logger.exception(
            f"Failed to generate LLM error page, falling back to minimal plain text template: {e}"
        )
        return await _minimal_fallback_response()
