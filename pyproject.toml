[project]
name = "http-llm-server"
version = "0.1.0"
description = "An HTTP server that lets an LLM handle everything - no routes, no controllers, just AI generating complete HTTP responses"
readme = "README.md"
license = "MIT"
requires-python = ">=3.13"
authors = [
    {name = "Timothy J Fontaine", email = "tjfontaine@gmail.com"}
]
keywords = ["llm", "http", "server", "ai", "openai"]
classifiers = [
    "Development Status :: 3 - Alpha",
    "Intended Audience :: Developers",
    "Programming Language :: Python :: 3",
    "Programming Language :: Python :: 3.13",
    "Topic :: Internet :: WWW/HTTP :: HTTP Servers",
]
dependencies = [
    "openai>=1.82.1",
    "aiohttp>=3.9.5",
    "ruff>=0.4.4",
    "python-json-logger>=2.0.0",
    "rich>=13.0.0",
    "pyyaml>=6.0.1",
    "uvicorn>=0.30.1",
    "python-dotenv>=1.0.1",
    "jinja2>=3.1.4",
    "openai-agents",
    "mcp",
]

[project.scripts]
start_server = "server:run_server"
