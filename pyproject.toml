[project]
name = "http-llm-server"
version = "0.1.0"
description = "An HTTP server that lets an LLM handle everything - no routes, no controllers, just AI generating complete HTTP responses"
readme = "README.md"
license = "MIT"
requires-python = ">=3.13"
authors = [
    {name = "Timothy J Fontaine", email = "tjfontaine@gmail.com"}
]
keywords = ["llm", "http", "server", "ai", "openai"]
classifiers = [
    "Development Status :: 3 - Alpha",
    "Intended Audience :: Developers",
    "Programming Language :: Python :: 3",
    "Programming Language :: Python :: 3.13",
    "Topic :: Internet :: WWW/HTTP :: HTTP Servers",
]
dependencies = [
    "openai>=1.90.0",
    "aiohttp>=3.12.13",
    "ruff>=0.12.0",
    "python-json-logger>=3.3.0",
    "rich>=14.0.0",
    "pyyaml>=6.0.2",
    "uvicorn>=0.34.3",
    "python-dotenv>=1.1.0",
    "jinja2>=3.1.6",
    "openai-agents>=0.0.19",
    "mcp>=1.9.4",
    "pydantic>=2.11.7",
    "pydantic-settings>=2.10.0",
]

[project.scripts]
start_server = "server:run_server"
