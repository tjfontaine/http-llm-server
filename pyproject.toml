[project]
name = "http-llm-server"
version = "0.1.0"
description = "An HTTP server that lets an LLM handle everything - no routes, no controllers, just AI generating complete HTTP responses"
readme = "README.md"
license = "MIT"
requires-python = ">=3.13"
authors = [
    {name = "Timothy J Fontaine", email = "tjfontaine@gmail.com"}
]
keywords = ["llm", "http", "server", "ai", "openai"]
classifiers = [
    "Development Status :: 3 - Alpha",
    "Intended Audience :: Developers",
    "Programming Language :: Python :: 3",
    "Programming Language :: Python :: 3.13",
    "Topic :: Internet :: WWW/HTTP :: HTTP Servers",
]
dependencies = [
    "openai>=1.90.0",
    "aiohttp>=3.12.13",
    "aiofiles>=24.1.0",
    "ruff>=0.12.0",
    "python-json-logger>=3.3.0",
    "rich>=14.0.0",
    "pyyaml>=6.0.2",
    "uvicorn>=0.34.3",
    "python-dotenv>=1.1.0",
    "jinja2>=3.1.6",
    "openai-agents==0.2.11",
    "pydantic>=2.11.7",
    "pydantic-settings>=2.10.0",
]

[project.scripts]
start_server = "src.app:create_app"

[tool.ruff]
line-length = 88
exclude = ["vendor"]

[tool.ruff.lint]
select = ["E", "F", "I"]

[tool.ruff.format]
exclude = ["vendor"]

[tool.uv.sources]
openai-agents = { path = "vendor/openai-agents-python" }
